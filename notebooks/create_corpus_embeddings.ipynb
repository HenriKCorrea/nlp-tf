{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961b6ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (4.53.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (0.33.1)\n",
      "Requirement already satisfied: omegaconf in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: datasets==2.16.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: tqdm in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.12.13)\n",
      "Requirement already satisfied: packaging in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.16.1) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: nbconvert in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (7.16.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (5.8.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (25.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (2.19.2)\n",
      "Requirement already satisfied: webencodings in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
      "Requirement already satisfied: decorator in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: stack_data in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert) (4.3.8)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (4.24.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.26.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat>=5.7->nbconvert) (4.14.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from beautifulsoup4->nbconvert) (2.7)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/henrique/source/repos/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - conda-forge\n",
      " - pytorch\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.1.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install python dependencies\n",
    "%pip install torch transformers huggingface_hub omegaconf datasets==2.16.1 tqdm \n",
    "# Optinal python packages for better user experience\n",
    "%pip install ipywidgets nbconvert\n",
    "# Install conda dependencies\n",
    "%conda install -c conda-forge -c pytorch -c nvidia faiss-gpu=1.11.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dca1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import omegaconf\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, AutoTokenizer, DPRConfig, GPT2TokenizerFast\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Setup external services authentication\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d1041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys_substring(ordered_dict: OrderedDict[str, Any], find_pattern, replace_pattern):\n",
    "    \"\"\"\n",
    "    Rename keys in an OrderedDict by replacing substring occurrences using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        ordered_dict: The OrderedDict to modify\n",
    "        find_pattern: The regex pattern to find in keys\n",
    "        replace_pattern: The replacement pattern (can include backreferences like \\\\1, \\\\2)\n",
    "    \n",
    "    Returns:\n",
    "        New Mapping with renamed keys\n",
    "    \"\"\"\n",
    "    new_dict = OrderedDict[str, Any]()\n",
    "    compiled_pattern = re.compile(find_pattern)\n",
    "    \n",
    "    for key, value in ordered_dict.items():\n",
    "        if not compiled_pattern.search(key):\n",
    "            continue\n",
    "            \n",
    "        new_key = compiled_pattern.sub(replace_pattern, key)\n",
    "        new_dict[new_key] = value\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f585fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the necessary types are registered for safe deserialization\n",
    "torch.serialization.add_safe_globals(\n",
    "    [\n",
    "        omegaconf.dictconfig.ContainerMetadata,\n",
    "        omegaconf.dictconfig.DictConfig,\n",
    "        omegaconf.base.Metadata,\n",
    "        omegaconf.nodes.AnyNode,\n",
    "        omegaconf.listconfig.ListConfig,\n",
    "        collections.defaultdict,\n",
    "        Any,\n",
    "        dict,\n",
    "        list,\n",
    "        int,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model from checkpoint\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"NTU-NLP-sg/xCodeEval-nl-code-starencoder-ckpt-37\",\n",
    "    filename=\"dpr_biencoder.37.pt\",\n",
    "    repo_type=\"model\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Retrieve fine-tuned weights\n",
    "# Pattern matches: question_model.embeddings/encoder.* -> question_encoder.bert_model.*\n",
    "ctx_state_dict = rename_keys_substring(\n",
    "    state_dict[\"model_dict\"],\n",
    "    r\"ctx_model\\.(embeddings|encoder)\\.([Ll]ayer|token|word|position_embeddings)\",\n",
    "    r\"ctx_encoder.bert_model.\\1.\\2\",\n",
    ")\n",
    "\n",
    "# Initialize encoders\n",
    "pretrained_model_name = state_dict[\"encoder_params\"][\"encoder\"][\"pretrained_model_cfg\"]\n",
    "encoder_config = DPRConfig.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\n",
    "    None, state_dict=ctx_state_dict, config=encoder_config, token=HF_TOKEN\n",
    ")\n",
    "ctx_encoder = ctx_encoder.to(device).eval()\n",
    "\n",
    "# Distribute workload across multiple GPUs if available\n",
    "# Not so efficient as DistributedDataParallel, but simpler for single-node setups\n",
    "# DataParallel will split the input across the GPUs and gather the outputs\n",
    "# This is useful for inference or when the model is not too large\n",
    "if torch.cuda.device_count() > 1:\n",
    "    ctx_encoder = torch.nn.DataParallel(ctx_encoder)\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for context encoder\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name, config=encoder_config\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd19b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage statistics\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()  # Ensure all GPU operations are complete\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "            reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "            print(f\"GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.2f}GB total\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU memory stats to report.\")\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Comprehensive memory cleanup\"\"\"\n",
    "    # Clear Python garbage\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Print memory stats\n",
    "    print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81a1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_codes(batch: DatasetDict[str, list]):\n",
    "    inputs = tokenizer(\n",
    "        batch[\"source_code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"  # Use PyTorch tensors \n",
    "    )\n",
    "    # Move to device in one operation\n",
    "    inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad(), torch.amp.autocast(device):\n",
    "        print(\"Embedding batch...\")\n",
    "        print_memory_usage()\n",
    "        embeddings = ctx_encoder(**inputs).pooler_output\n",
    "        print(\"Batch embedded.\")\n",
    "        print_memory_usage()\n",
    "        # Convert to float16 to save memory if precision allows\n",
    "        embeddings_cpu = embeddings.cpu().numpy().astype(np.float32).tolist()\n",
    "        return {\"embedding\": embeddings_cpu}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80986ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: 3.89GB allocated, 17.68GB reserved, 19.57GB total\n",
      "GPU 1: 0.00GB allocated, 0.00GB reserved, 19.57GB total\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c05994f2a9b4257a143259e6fcfd8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25043700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "GPU 0: 3.90GB allocated, 17.68GB reserved, 19.57GB total\n",
      "GPU 1: 0.00GB allocated, 0.00GB reserved, 19.57GB total\n",
      "Batch embedded.\n",
      "GPU 0: 4.77GB allocated, 19.01GB reserved, 19.57GB total\n",
      "GPU 1: 0.00GB allocated, 0.00GB reserved, 19.57GB total\n",
      "Embedding batch...\n",
      "GPU 0: 3.90GB allocated, 19.01GB reserved, 19.57GB total\n",
      "GPU 1: 0.00GB allocated, 0.00GB reserved, 19.57GB total\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.35 GiB. GPU 0 has a total capacity of 19.57 GiB of which 282.00 MiB is free. Including non-PyTorch memory, this process has 19.29 GiB memory in use. Of the allocated memory 15.10 GiB is allocated by PyTorch, and 3.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m\n",
      "\u001b[32m     10\u001b[39m corpus = load_dataset(\n",
      "\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNTU-NLP-sg/xCodeEval\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretrieval_corpus\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     token=HF_TOKEN,\n",
      "\u001b[32m     17\u001b[39m )\n",
      "\u001b[32m     19\u001b[39m print_memory_usage()\n",
      "\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m corpus = \u001b[43mcorpus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_codes\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m230\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase batch size to compensate for no multiprocessing\u001b[39;49;00m\n",
      "\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# remove_columns=[col for col in corpus.column_names if col != \"source_code\"]  # Remove unused columns\u001b[39;49;00m\n",
      "\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     27\u001b[39m i = \u001b[32m0\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:592\u001b[39m, in \u001b[36mtransmit_tasks.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    593\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n",
      "\u001b[32m    594\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n",
      "\u001b[32m    595\u001b[39m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    550\u001b[39m self_format = {\n",
      "\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n",
      "\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n",
      "\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n",
      "\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n",
      "\u001b[32m    555\u001b[39m }\n",
      "\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    558\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n",
      "\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3093\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n",
      "\u001b[32m   3087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m   3088\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n",
      "\u001b[32m   3089\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   3090\u001b[39m         total=pbar_total,\n",
      "\u001b[32m   3091\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m   3092\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset._map_single(**dataset_kwargs):\n",
      "\u001b[32m   3094\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "\u001b[32m   3095\u001b[39m                 shards_done += \u001b[32m1\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3470\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n",
      "\u001b[32m   3466\u001b[39m indices = \u001b[38;5;28mlist\u001b[39m(\n",
      "\u001b[32m   3467\u001b[39m     \u001b[38;5;28mrange\u001b[39m(*(\u001b[38;5;28mslice\u001b[39m(i, i + batch_size).indices(shard.num_rows)))\n",
      "\u001b[32m   3468\u001b[39m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n",
      "\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3470\u001b[39m     batch = \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   3471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   3472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   3473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   3474\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   3475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3476\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n",
      "\u001b[32m   3477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n",
      "\u001b[32m   3478\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   3479\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3349\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[39m\u001b[34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[39m\n",
      "\u001b[32m   3347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n",
      "\u001b[32m   3348\u001b[39m     additional_args += (rank,)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3349\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n",
      "\u001b[32m   3351\u001b[39m     processed_inputs = {\n",
      "\u001b[32m   3352\u001b[39m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs.data.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs.keys_to_format\n",
      "\u001b[32m   3353\u001b[39m     }\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36membed_codes\u001b[39m\u001b[34m(batch)\u001b[39m\n",
      "\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbedding batch...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     14\u001b[39m print_memory_usage()\n",
      "\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m embeddings = \u001b[43mctx_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.pooler_output\n",
      "\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch embedded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     17\u001b[39m print_memory_usage()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/dpr/modeling_dpr.py:369\u001b[39m, in \u001b[36mDPRContextEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    367\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[32m1\u001b[39m:]\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/dpr/modeling_dpr.py:149\u001b[39m, in \u001b[36mDPREncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n",
      "\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[32m    141\u001b[39m     input_ids: Tensor,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     return_dict: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[32m    148\u001b[39m ) -> Union[BaseModelOutputWithPooling, \u001b[38;5;28mtuple\u001b[39m[Tensor, ...]]:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    158\u001b[39m     sequence_output = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m    159\u001b[39m     pooled_output = sequence_output[:, \u001b[32m0\u001b[39m, :]\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n",
      "\u001b[32m    990\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n",
      "\u001b[32m    991\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n",
      "\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n",
      "\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n",
      "\u001b[32m    994\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1008\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m   1009\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:651\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    648\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    649\u001b[39m past_key_value = past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n",
      "\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    659\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    661\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m     80\u001b[39m         logger.warning(message)\n",
      "\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:595\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n",
      "\u001b[32m    592\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n",
      "\u001b[32m    593\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n",
      "\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n",
      "\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    598\u001b[39m outputs = (layer_output,) + outputs\n",
      "\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/pytorch_utils.py:250\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n",
      "\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n",
      "\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n",
      "\u001b[32m    606\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    608\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[32m    609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:508\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n",
      "\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n",
      "\u001b[32m    507\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/transformers/activations.py:69\u001b[39m, in \u001b[36mGELUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n",
      "\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.35 GiB. GPU 0 has a total capacity of 19.57 GiB of which 282.00 MiB is free. Including non-PyTorch memory, this process has 19.29 GiB memory in use. Of the allocated memory 15.10 GiB is allocated by PyTorch, and 3.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load corpus dataset\n",
    "corpus = load_dataset(\n",
    "    \"NTU-NLP-sg/xCodeEval\",\n",
    "    \"retrieval_corpus\",\n",
    "    trust_remote_code=True,\n",
    "    split=\"test\",\n",
    "    revision=\"467d25a839086383794b58055981221b82c0d107\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "corpus = corpus.map(\n",
    "    embed_codes,\n",
    "    batched=True,\n",
    "    batch_size=128,\n",
    ")\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e907eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
