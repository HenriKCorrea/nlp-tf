{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "961b6ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (4.53.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (0.33.2)\n",
      "Requirement already satisfied: omegaconf in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: datasets==2.16.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.12.13)\n",
      "Requirement already satisfied: packaging in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.16.1) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: nbconvert in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (7.16.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (5.8.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (25.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (2.19.2)\n",
      "Requirement already satisfied: webencodings in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
      "Requirement already satisfied: decorator in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: stack_data in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert) (4.3.8)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (4.24.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.26.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat>=5.7->nbconvert) (4.14.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from beautifulsoup4->nbconvert) (2.7)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install python dependencies\n",
    "%pip install torch transformers huggingface_hub omegaconf datasets==2.16.1 \n",
    "# Optinal python packages for better user experience\n",
    "%pip install ipywidgets nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dca1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import omegaconf\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "from transformers import DPRContextEncoder, AutoTokenizer, DPRConfig, GPT2TokenizerFast\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from concurrent.futures import ProcessPoolExecutor, wait\n",
    "import time\n",
    "\n",
    "# Setup external services authentication\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Configure cache settings\n",
    "CACHE_DIR = Path(\"./cache\")\n",
    "CORPUS_CACHE_DIR = CACHE_DIR / \"corpus_embeddings\"\n",
    "CORPUS_CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CORPUS_CACHE_DATASET_DIR = CORPUS_CACHE_DIR / \"dataset\"\n",
    "CORPUS_CACHE_DATASET_DIR.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d1041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys_substring(ordered_dict: OrderedDict[str, Any], find_pattern, replace_pattern):\n",
    "    \"\"\"\n",
    "    Rename keys in an OrderedDict by replacing substring occurrences using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        ordered_dict: The OrderedDict to modify\n",
    "        find_pattern: The regex pattern to find in keys\n",
    "        replace_pattern: The replacement pattern (can include backreferences like \\\\1, \\\\2)\n",
    "    \n",
    "    Returns:\n",
    "        New Mapping with renamed keys\n",
    "    \"\"\"\n",
    "    new_dict = OrderedDict[str, Any]()\n",
    "    compiled_pattern = re.compile(find_pattern)\n",
    "    \n",
    "    for key, value in ordered_dict.items():\n",
    "        if not compiled_pattern.search(key):\n",
    "            continue\n",
    "            \n",
    "        new_key = compiled_pattern.sub(replace_pattern, key)\n",
    "        new_dict[new_key] = value\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c034b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_on_device(device: str) -> tuple[DPRContextEncoder, GPT2TokenizerFast]:\n",
    "    \"\"\"\n",
    "    Setup model on the specified device.\n",
    "\n",
    "    Args:\n",
    "        device: Device to load the model on, either 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the context encoder and tokenizer.\n",
    "    \"\"\"\n",
    "    torch.serialization.add_safe_globals(\n",
    "        [\n",
    "            omegaconf.dictconfig.ContainerMetadata,\n",
    "            omegaconf.dictconfig.DictConfig,\n",
    "            omegaconf.base.Metadata,\n",
    "            omegaconf.nodes.AnyNode,\n",
    "            omegaconf.listconfig.ListConfig,\n",
    "            collections.defaultdict,\n",
    "            Any,\n",
    "            dict,\n",
    "            list,\n",
    "            int,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load model state dict (shared across all GPUs)\n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=\"NTU-NLP-sg/xCodeEval-nl-code-starencoder-ckpt-37\",\n",
    "        filename=\"dpr_biencoder.37.pt\",\n",
    "        repo_type=\"model\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Retrieve fine-tuned weights\n",
    "    ctx_state_dict = rename_keys_substring(\n",
    "        state_dict[\"model_dict\"],\n",
    "        r\"ctx_model\\.(embeddings|encoder)\\.([Ll]ayer|token|word|position_embeddings)\",\n",
    "        r\"ctx_encoder.bert_model.\\1.\\2\",\n",
    "    )\n",
    "\n",
    "    # Initialize encoder\n",
    "    pretrained_model_name = state_dict[\"encoder_params\"][\"encoder\"][\n",
    "        \"pretrained_model_cfg\"\n",
    "    ]\n",
    "    encoder_config = DPRConfig.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(\n",
    "        None, state_dict=ctx_state_dict, config=encoder_config, token=HF_TOKEN\n",
    "    )\n",
    "    ctx_encoder = ctx_encoder.to(device).eval()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name, config=encoder_config\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return ctx_encoder, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13979160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard_on_gpu(process_id: int, shard: Dataset) -> str:\n",
    "    \"\"\"\n",
    "    Process a single shard of the dataset on the specified GPU.\n",
    "    \n",
    "    Args:\n",
    "        process_id: The ID of the GPU to use for processing\n",
    "        shard: The dataset shard to process\n",
    "    \n",
    "    Returns:\n",
    "        Path to Dataset with embeddings added\n",
    "    \"\"\"\n",
    "    print(f\"Process {process_id}: Starting processing of {len(shard)} documents\")\n",
    "    # Set device for this process\n",
    "    deviceType = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = f\"{deviceType}:{process_id % torch.cuda.device_count()}\"  # Use modulo to handle multiple GPUs\n",
    "    \n",
    "    # Load model on this specific GPU\n",
    "    ctx_encoder_gpu, tokenizer_gpu = setup_model_on_device(device)\n",
    "    \n",
    "    # Create embedding function for this GPU\n",
    "    def embed_codes_gpu(batch):\n",
    "        inputs = tokenizer_gpu(\n",
    "            batch[\"source_code\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "        # bfloat16 is more memory efficient on GPUs like RTX 3090\n",
    "        # but has a lower precision than float32\n",
    "        # bfloat16: 16 bits, 1 sign bit, 8 exponent bits, 7 mantissa bits\n",
    "        # float16: 16 bits, 1 sign bit, 5 exponent bits, 10 mantissa bits\n",
    "        # float32: 32 bits, 1 sign bit, 8 exponent bits, 23 mantissa bits\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=deviceType, dtype=torch.bfloat16):\n",
    "            embeddings = ctx_encoder_gpu(**inputs).pooler_output\n",
    "            embeddings_cpu = embeddings.detach().cpu().to(torch.float32).tolist()\n",
    "            # print(f\"Process {process_id}: GPU memory usage: {torch.cuda.memory_allocated(device) / (1024 ** 3):.2f} GB\")\n",
    "            # print(f\"Process {process_id}: GPU allocated memory: {torch.cuda.memory_reserved(device) / (1024 ** 3):.2f} GB\")\n",
    "            return {\"embedding\": embeddings_cpu}\n",
    "    \n",
    "    # Process the shard\n",
    "    try:\n",
    "        processed_shard = shard.map(\n",
    "            embed_codes_gpu,\n",
    "            batched=True,\n",
    "            batch_size=48,\n",
    "            desc=f\"Process {process_id}\",\n",
    "        )\n",
    "        print(f\"Process {process_id}: Successfully processed {len(shard)} documents\")\n",
    "    \n",
    "        # Save the processed dataset to a specific location for later loading\n",
    "        print(f\"Process {process_id}: Saving processed shard to disk\")\n",
    "        shard_directory = CORPUS_CACHE_DIR / f\"shard_{process_id}\"\n",
    "        shard_directory.mkdir(parents=True, exist_ok=True)\n",
    "        processed_shard.save_to_disk(str(shard_directory))\n",
    "\n",
    "        # Ensure shard directory has content\n",
    "        if not any(shard_directory.iterdir()):\n",
    "            raise FileNotFoundError(f\"Processed shard file not found: {shard_directory}\")\n",
    "        \n",
    "        return str(shard_directory)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Process {process_id}: Error during processing: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93421bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_processpool(corpus: Dataset):\n",
    "    \"\"\"\n",
    "    Process the dataset using a process pool for true parallel execution.\n",
    "    Each process gets its own CUDA context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    num_processes = num_gpus * 2  # Use more GPUs per process for better load balancing\n",
    "    print(f\"Found {num_gpus} GPUs available\")\n",
    "    \n",
    "    if num_gpus < 1:\n",
    "        raise RuntimeError(\"At least one GPU is required for this operation.\")\n",
    "\n",
    "    # Calculate shard sizes\n",
    "    total_docs = len(corpus)\n",
    "    docs_per_process = total_docs // num_processes\n",
    "    remainder = total_docs % num_processes\n",
    "\n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Documents per process: {docs_per_process}\")\n",
    "    print(f\"Remainder documents: {remainder}\")\n",
    "    \n",
    "    # Create shards and distribute workload across Processes\n",
    "    shards = []\n",
    "    start_idx = 0\n",
    "    for process_id in range(num_processes):\n",
    "        # Give remainder documents to first few processes\n",
    "        shard_size = docs_per_process + (1 if process_id < remainder else 0)\n",
    "        end_idx = start_idx + shard_size\n",
    "        \n",
    "        shard = corpus.select(range(start_idx, end_idx))\n",
    "        shards.append((process_id, shard))\n",
    "\n",
    "        print(f\"Process {process_id}: Processing documents {start_idx} to {end_idx-1} ({shard_size} docs)\")\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Process shards in parallel using processes\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        futures = []\n",
    "        for process_id, shard in shards:\n",
    "            future = executor.submit(process_shard_on_gpu, process_id, shard)\n",
    "            futures.append((process_id, future))\n",
    "\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Starting parallel processing on {num_processes} Processes...\")        \n",
    "        # Extract just the futures for wait()\n",
    "        future_objects = [future for process_id, future in futures]\n",
    "        awaited_future_objects = wait(future_objects)  # Wait for all futures to complete\n",
    "    \n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] All Processes completed processing!\")\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Please wait for the main process to combine results...\")    \n",
    "        shard_results: list[Dataset] = []\n",
    "        for process_id, future in futures:\n",
    "            shard_dataset_path: str = future.result()\n",
    "            shard_results.append(Dataset.load_from_disk(shard_dataset_path))\n",
    "    \n",
    "    # Combine all shard results\n",
    "    print(\"Combining results from all Processes...\")\n",
    "\n",
    "    # Create final dataset\n",
    "    corpus_with_embeddings = concatenate_datasets(shard_results)\n",
    "    \n",
    "    print(f\"Combined dataset created with {len(corpus_with_embeddings)} documents\")\n",
    "    return corpus_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30cd9f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache found. Processing corpus...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 GPUs available\n",
      "Total documents: 25043700\n",
      "Documents per process: 3130462\n",
      "Remainder documents: 4\n",
      "Process 0: Processing documents 0 to 3130462 (3130463 docs)\n",
      "Process 1: Processing documents 3130463 to 6260925 (3130463 docs)\n",
      "Process 2: Processing documents 6260926 to 9391388 (3130463 docs)\n",
      "Process 3: Processing documents 9391389 to 12521851 (3130463 docs)\n",
      "Process 4: Processing documents 12521852 to 15652313 (3130462 docs)\n",
      "Process 5: Processing documents 15652314 to 18782775 (3130462 docs)\n",
      "Process 6: Processing documents 18782776 to 21913237 (3130462 docs)\n",
      "Process 7: Processing documents 21913238 to 25043699 (3130462 docs)\n",
      "Process 0: Starting processing of 3130463 documents\n",
      "Process 2: Starting processing of 3130463 documents\n",
      "Process 1: Starting processing of 3130463 documentsProcess 4: Starting processing of 3130462 documents\n",
      "\n",
      "Process 6: Starting processing of 3130462 documents\n",
      "Process 5: Starting processing of 3130462 documentsProcess 3: Starting processing of 3130463 documents\n",
      "\n",
      "Process 7: Starting processing of 3130462 documents\n",
      "[12:39:34] Starting parallel processing on 8 Processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d77a03e9276415cbd4571293f8e67f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 4:   0%|          | 0/3130462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47b34116c6f42ac808640f837440027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 2:   0%|          | 0/3130463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c277dfd8e11d460a98501c55dd5c2ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 6:   0%|          | 0/3130462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8658bfa68441239270113e897c8e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 3:   0%|          | 0/3130463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309a163c18b84c67af1b93d808594282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 5:   0%|          | 0/3130462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f95ba9a086448fbbf6bdcf2ae117fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 1:   0%|          | 0/3130463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ba818cbe2a48b893dc3c51cf827aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 7:   0%|          | 0/3130462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944c320b2bb44416a1a3253b1217110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Process 0:   0%|          | 0/3130463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     18\u001b[39m corpus = load_dataset(\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNTU-NLP-sg/xCodeEval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretrieval_corpus\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     token=HF_TOKEN,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m corpus_with_embeddings = \u001b[43mprocess_with_processpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbeddings generated successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving corpus cache to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCORPUS_CACHE_DATASET_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mprocess_with_processpool\u001b[39m\u001b[34m(corpus)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Extract just the futures for wait()\u001b[39;00m\n\u001b[32m     47\u001b[39m future_objects = [future \u001b[38;5;28;01mfor\u001b[39;00m process_id, future \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m awaited_future_objects = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_objects\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for all futures to complete\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] All Processes completed processing!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Please wait for the main process to combine results...\u001b[39m\u001b[33m\"\u001b[39m)    \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/concurrent/futures/_base.py:305\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    301\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[32m    303\u001b[39m     waiter = _create_and_install_waiters(fs, return_when)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m f._condition:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Check if cache exists and load, otherwise process corpus\n",
    "if any(CORPUS_CACHE_DATASET_DIR.iterdir()):\n",
    "    try:\n",
    "        print(f\"Loading corpus cache from {CORPUS_CACHE_DATASET_DIR}\")\n",
    "        corpus_with_embeddings = Dataset.load_from_disk(str(CORPUS_CACHE_DATASET_DIR))\n",
    "        print(f\"Cache loaded successfully. Documents: {len(corpus_with_embeddings)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load cache: {e}\")\n",
    "        print(\"Cache directory exists but contains invalid data. Recreating cache...\")\n",
    "        corpus_with_embeddings = None\n",
    "else:\n",
    "    corpus_with_embeddings = None\n",
    "\n",
    "if corpus_with_embeddings is None:\n",
    "    print(\"No cache found. Processing corpus...\")\n",
    "    \n",
    "    # Load corpus dataset\n",
    "    corpus = load_dataset(\n",
    "        \"NTU-NLP-sg/xCodeEval\",\n",
    "        \"retrieval_corpus\",\n",
    "        trust_remote_code=True,\n",
    "        split=\"test\",\n",
    "        revision=\"467d25a839086383794b58055981221b82c0d107\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    corpus_with_embeddings = process_with_processpool(corpus)\n",
    "\n",
    "    print(\"Embeddings generated successfully!\")\n",
    "    print(f\"Saving corpus cache to {CORPUS_CACHE_DATASET_DIR}\")\n",
    "    corpus_with_embeddings.save_to_disk(str(CORPUS_CACHE_DATASET_DIR))\n",
    "    print(\"Cache saved successfully!\")\n",
    "\n",
    "# Display information about the processed corpus\n",
    "print(f\"\\nCorpus information:\")\n",
    "print(f\"Number of documents: {len(corpus_with_embeddings)}\")\n",
    "if len(corpus_with_embeddings) > 0:\n",
    "    print(f\"Embedding dimension: {len(corpus_with_embeddings[0]['embedding'])}\")\n",
    "    print(f\"Sample document keys: {list(corpus_with_embeddings[0].keys())}\")\n",
    "    print(f\"Sample source code (first 200 chars): {corpus_with_embeddings[0]['source_code'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
