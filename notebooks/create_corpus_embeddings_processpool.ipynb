{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python dependencies\n",
    "%pip install torch transformers huggingface_hub omegaconf datasets==2.16.1 \n",
    "# Optinal python packages for better user experience\n",
    "%pip install ipywidgets nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import omegaconf\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "from transformers import DPRContextEncoder, AutoTokenizer, DPRConfig, GPT2TokenizerFast\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, Dataset\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Setup external services authentication\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Configure cache settings\n",
    "CACHE_DIR = Path(\"./cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "CORPUS_CACHE_DIR = CACHE_DIR / \"corpus_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys_substring(ordered_dict: OrderedDict[str, Any], find_pattern, replace_pattern):\n",
    "    \"\"\"\n",
    "    Rename keys in an OrderedDict by replacing substring occurrences using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        ordered_dict: The OrderedDict to modify\n",
    "        find_pattern: The regex pattern to find in keys\n",
    "        replace_pattern: The replacement pattern (can include backreferences like \\\\1, \\\\2)\n",
    "    \n",
    "    Returns:\n",
    "        New Mapping with renamed keys\n",
    "    \"\"\"\n",
    "    new_dict = OrderedDict[str, Any]()\n",
    "    compiled_pattern = re.compile(find_pattern)\n",
    "    \n",
    "    for key, value in ordered_dict.items():\n",
    "        if not compiled_pattern.search(key):\n",
    "            continue\n",
    "            \n",
    "        new_key = compiled_pattern.sub(replace_pattern, key)\n",
    "        new_dict[new_key] = value\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c034b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_on_device(device: str) -> tuple[DPRContextEncoder, GPT2TokenizerFast]:\n",
    "    \"\"\"\n",
    "    Setup model on the specified device.\n",
    "\n",
    "    Args:\n",
    "        device: Device to load the model on, either 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the context encoder and tokenizer.\n",
    "    \"\"\"\n",
    "    torch.serialization.add_safe_globals(\n",
    "        [\n",
    "            omegaconf.dictconfig.ContainerMetadata,\n",
    "            omegaconf.dictconfig.DictConfig,\n",
    "            omegaconf.base.Metadata,\n",
    "            omegaconf.nodes.AnyNode,\n",
    "            omegaconf.listconfig.ListConfig,\n",
    "            collections.defaultdict,\n",
    "            Any,\n",
    "            dict,\n",
    "            list,\n",
    "            int,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load model state dict (shared across all GPUs)\n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=\"NTU-NLP-sg/xCodeEval-nl-code-starencoder-ckpt-37\",\n",
    "        filename=\"dpr_biencoder.37.pt\",\n",
    "        repo_type=\"model\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Retrieve fine-tuned weights\n",
    "    ctx_state_dict = rename_keys_substring(\n",
    "        state_dict[\"model_dict\"],\n",
    "        r\"ctx_model\\.(embeddings|encoder)\\.([Ll]ayer|token|word|position_embeddings)\",\n",
    "        r\"ctx_encoder.bert_model.\\1.\\2\",\n",
    "    )\n",
    "\n",
    "    # Initialize encoder\n",
    "    pretrained_model_name = state_dict[\"encoder_params\"][\"encoder\"][\n",
    "        \"pretrained_model_cfg\"\n",
    "    ]\n",
    "    encoder_config = DPRConfig.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(\n",
    "        None, state_dict=ctx_state_dict, config=encoder_config, token=HF_TOKEN\n",
    "    )\n",
    "    ctx_encoder = ctx_encoder.to(device).eval()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name, config=encoder_config\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return ctx_encoder, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13979160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard_on_gpu(gpu_id: int, shard: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Process a single shard of the dataset on the specified GPU.\n",
    "    \n",
    "    Args:\n",
    "        gpu_id: The ID of the GPU to use for processing\n",
    "        shard: The dataset shard to process\n",
    "    \n",
    "    Returns:\n",
    "        Dataset with embeddings added\n",
    "    \"\"\"\n",
    "    print(f\"GPU {gpu_id}: Starting processing of {len(shard)} documents\")\n",
    "    # Set device for this process\n",
    "    deviceType = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = f\"{deviceType}:{gpu_id}\"\n",
    "    \n",
    "    # Load model on this specific GPU\n",
    "    ctx_encoder_gpu, tokenizer_gpu = setup_model_on_device(device)\n",
    "    \n",
    "    # Create embedding function for this GPU\n",
    "    def embed_codes_gpu(batch):\n",
    "        inputs = tokenizer_gpu(\n",
    "            batch[\"source_code\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "        # bfloat16 is more memory efficient on GPUs like RTX 3090\n",
    "        # but has a lower precision than float32\n",
    "        # bfloat16: 16 bits, 1 sign bit, 8 exponent bits, 7 mantissa bits\n",
    "        # float16: 16 bits, 1 sign bit, 5 exponent bits, 10 mantissa bits\n",
    "        # float32: 32 bits, 1 sign bit, 8 exponent bits, 23 mantissa bits\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=deviceType, dtype=torch.bfloat16):\n",
    "            embeddings = ctx_encoder_gpu(**inputs).pooler_output\n",
    "            # torch.cuda.synchronize()\n",
    "            embeddings_cpu = embeddings.detach().cpu().to(torch.float32).tolist()\n",
    "            return {\"embedding\": embeddings_cpu}\n",
    "    \n",
    "    # Process the shard\n",
    "    try:\n",
    "        shard_with_embeddings = shard.map(\n",
    "            embed_codes_gpu,\n",
    "            batched=True,\n",
    "            batch_size=48,\n",
    "            desc=f\"GPU {gpu_id}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"GPU {gpu_id}: Successfully processed {len(shard_with_embeddings)} documents\")\n",
    "        return shard_with_embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GPU {gpu_id}: Error during processing: {e}\")\n",
    "        raise e\n",
    "    # finally:\n",
    "        # Clean up GPU memory\n",
    "        # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93421bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_processpool(corpus: Dataset):\n",
    "    \"\"\"\n",
    "    Process the dataset using a process pool for true parallel execution.\n",
    "    Each process gets its own CUDA context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} GPUs available\")\n",
    "    \n",
    "    if num_gpus < 1:\n",
    "        raise RuntimeError(\"At least one GPU is required for this operation.\")\n",
    "\n",
    "    # Calculate shard sizes\n",
    "    total_docs = len(corpus)\n",
    "    docs_per_gpu = total_docs // num_gpus\n",
    "    remainder = total_docs % num_gpus\n",
    "    \n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Documents per GPU: {docs_per_gpu}\")\n",
    "    print(f\"Remainder documents: {remainder}\")\n",
    "    \n",
    "    # Create shards and distribute workload across GPUs\n",
    "    shards = []\n",
    "    start_idx = 0\n",
    "    for gpu_id in range(num_gpus):\n",
    "        # Give remainder documents to first few GPUs\n",
    "        shard_size = docs_per_gpu + (1 if gpu_id < remainder else 0)\n",
    "        end_idx = start_idx + shard_size\n",
    "        \n",
    "        shard = corpus.select(range(start_idx, end_idx))\n",
    "        shards.append((gpu_id, shard))\n",
    "        \n",
    "        print(f\"GPU {gpu_id}: Processing documents {start_idx} to {end_idx-1} ({shard_size} docs)\")\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Process shards in parallel using processes\n",
    "    with ProcessPoolExecutor(max_workers=num_gpus) as executor:\n",
    "        futures = []\n",
    "        for gpu_id, shard in shards:\n",
    "            future = executor.submit(process_shard_on_gpu, gpu_id, shard)\n",
    "            futures.append((gpu_id, future))\n",
    "        \n",
    "        # Monitor progress\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Starting parallel processing on {num_gpus} GPUs...\")\n",
    "        \n",
    "        shard_results = [None] * num_gpus\n",
    "        completed_count = 0\n",
    "        \n",
    "        for future in as_completed([f for _, f in futures]):\n",
    "            # Find which GPU this future belongs to\n",
    "            gpu_id = next(gid for gid, f in futures if f is future)\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                shard_results[gpu_id] = result\n",
    "                completed_count += 1\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] GPU {gpu_id} completed! ({completed_count}/{num_gpus} GPUs finished)\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] GPU {gpu_id} failed: {e}\")\n",
    "                raise e\n",
    "        \n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] All GPUs completed processing!\")\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Please wait for the main process to combine results...\")\n",
    "    \n",
    "    # Combine all shard results\n",
    "    print(\"Combining results from all GPUs...\")\n",
    "    combined_data = {}\n",
    "    \n",
    "    # Get all keys from first shard\n",
    "    first_shard = shard_results[0]\n",
    "    for key in first_shard.features.keys():\n",
    "        combined_data[key] = []\n",
    "    \n",
    "    # Combine data from all shards\n",
    "    for shard_result in shard_results:\n",
    "        for key in combined_data.keys():\n",
    "            combined_data[key].extend(shard_result[key])\n",
    "    \n",
    "    # Create final dataset\n",
    "    corpus_with_embeddings = Dataset.from_dict(combined_data)\n",
    "    \n",
    "    print(f\"Combined dataset created with {len(corpus_with_embeddings)} documents\")\n",
    "    return corpus_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cache exists and load, otherwise process corpus\n",
    "if CORPUS_CACHE_DIR.exists():\n",
    "    try:\n",
    "        print(f\"Loading corpus cache from {CORPUS_CACHE_DIR}\")\n",
    "        corpus_with_embeddings = Dataset.load_from_disk(str(CORPUS_CACHE_DIR))\n",
    "        print(f\"Cache loaded successfully. Documents: {len(corpus_with_embeddings)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load cache: {e}\")\n",
    "        print(\"Cache directory exists but contains invalid data. Recreating cache...\")\n",
    "        corpus_with_embeddings = None\n",
    "else:\n",
    "    corpus_with_embeddings = None\n",
    "\n",
    "if corpus_with_embeddings is None:\n",
    "    print(\"No cache found. Processing corpus...\")\n",
    "    \n",
    "    # Load corpus dataset\n",
    "    corpus = load_dataset(\n",
    "        \"NTU-NLP-sg/xCodeEval\",\n",
    "        \"retrieval_corpus\",\n",
    "        trust_remote_code=True,\n",
    "        split=\"test\",\n",
    "        revision=\"467d25a839086383794b58055981221b82c0d107\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    corpus_with_embeddings = process_with_processpool(corpus)\n",
    "    \n",
    "    print(\"Embeddings generated successfully!\")\n",
    "    print(f\"Saving corpus cache to {CORPUS_CACHE_DIR}\")\n",
    "    corpus_with_embeddings.save_to_disk(str(CORPUS_CACHE_DIR))\n",
    "    print(\"Cache saved successfully!\")\n",
    "\n",
    "# Display information about the processed corpus\n",
    "print(f\"\\nCorpus information:\")\n",
    "print(f\"Number of documents: {len(corpus_with_embeddings)}\")\n",
    "if len(corpus_with_embeddings) > 0:\n",
    "    print(f\"Embedding dimension: {len(corpus_with_embeddings[0]['embedding'])}\")\n",
    "    print(f\"Sample document keys: {list(corpus_with_embeddings[0].keys())}\")\n",
    "    print(f\"Sample source code (first 200 chars): {corpus_with_embeddings[0]['source_code'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
