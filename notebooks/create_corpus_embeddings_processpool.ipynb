{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python dependencies\n",
    "%pip install torch transformers huggingface_hub omegaconf datasets==2.16.1 \n",
    "# Optinal python packages for better user experience\n",
    "%pip install ipywidgets nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import omegaconf\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "from transformers import DPRContextEncoder, AutoTokenizer, DPRConfig, GPT2TokenizerFast\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from concurrent.futures import ProcessPoolExecutor, wait\n",
    "import time\n",
    "\n",
    "# Setup external services authentication\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Configure cache settings\n",
    "CACHE_DIR = Path(\"./cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "CORPUS_CACHE_DIR = CACHE_DIR / \"corpus_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys_substring(ordered_dict: OrderedDict[str, Any], find_pattern, replace_pattern):\n",
    "    \"\"\"\n",
    "    Rename keys in an OrderedDict by replacing substring occurrences using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        ordered_dict: The OrderedDict to modify\n",
    "        find_pattern: The regex pattern to find in keys\n",
    "        replace_pattern: The replacement pattern (can include backreferences like \\\\1, \\\\2)\n",
    "    \n",
    "    Returns:\n",
    "        New Mapping with renamed keys\n",
    "    \"\"\"\n",
    "    new_dict = OrderedDict[str, Any]()\n",
    "    compiled_pattern = re.compile(find_pattern)\n",
    "    \n",
    "    for key, value in ordered_dict.items():\n",
    "        if not compiled_pattern.search(key):\n",
    "            continue\n",
    "            \n",
    "        new_key = compiled_pattern.sub(replace_pattern, key)\n",
    "        new_dict[new_key] = value\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c034b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_on_device(device: str) -> tuple[DPRContextEncoder, GPT2TokenizerFast]:\n",
    "    \"\"\"\n",
    "    Setup model on the specified device.\n",
    "\n",
    "    Args:\n",
    "        device: Device to load the model on, either 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing the context encoder and tokenizer.\n",
    "    \"\"\"\n",
    "    torch.serialization.add_safe_globals(\n",
    "        [\n",
    "            omegaconf.dictconfig.ContainerMetadata,\n",
    "            omegaconf.dictconfig.DictConfig,\n",
    "            omegaconf.base.Metadata,\n",
    "            omegaconf.nodes.AnyNode,\n",
    "            omegaconf.listconfig.ListConfig,\n",
    "            collections.defaultdict,\n",
    "            Any,\n",
    "            dict,\n",
    "            list,\n",
    "            int,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load model state dict (shared across all GPUs)\n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=\"NTU-NLP-sg/xCodeEval-nl-code-starencoder-ckpt-37\",\n",
    "        filename=\"dpr_biencoder.37.pt\",\n",
    "        repo_type=\"model\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Retrieve fine-tuned weights\n",
    "    ctx_state_dict = rename_keys_substring(\n",
    "        state_dict[\"model_dict\"],\n",
    "        r\"ctx_model\\.(embeddings|encoder)\\.([Ll]ayer|token|word|position_embeddings)\",\n",
    "        r\"ctx_encoder.bert_model.\\1.\\2\",\n",
    "    )\n",
    "\n",
    "    # Initialize encoder\n",
    "    pretrained_model_name = state_dict[\"encoder_params\"][\"encoder\"][\n",
    "        \"pretrained_model_cfg\"\n",
    "    ]\n",
    "    encoder_config = DPRConfig.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "    ctx_encoder = DPRContextEncoder.from_pretrained(\n",
    "        None, state_dict=ctx_state_dict, config=encoder_config, token=HF_TOKEN\n",
    "    )\n",
    "    ctx_encoder = ctx_encoder.to(device).eval()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name, config=encoder_config\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return ctx_encoder, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13979160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard_on_gpu(process_id: int, shard: Dataset) -> str:\n",
    "    \"\"\"\n",
    "    Process a single shard of the dataset on the specified GPU.\n",
    "    \n",
    "    Args:\n",
    "        process_id: The ID of the GPU to use for processing\n",
    "        shard: The dataset shard to process\n",
    "    \n",
    "    Returns:\n",
    "        Path to Dataset with embeddings added\n",
    "    \"\"\"\n",
    "    print(f\"Process {process_id}: Starting processing of {len(shard)} documents\")\n",
    "    # Set device for this process\n",
    "    deviceType = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = f\"{deviceType}:{process_id % torch.cuda.device_count()}\"  # Use modulo to handle multiple GPUs\n",
    "    \n",
    "    # Load model on this specific GPU\n",
    "    ctx_encoder_gpu, tokenizer_gpu = setup_model_on_device(device)\n",
    "    \n",
    "    # Create embedding function for this GPU\n",
    "    def embed_codes_gpu(batch):\n",
    "        inputs = tokenizer_gpu(\n",
    "            batch[\"source_code\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "        # bfloat16 is more memory efficient on GPUs like RTX 3090\n",
    "        # but has a lower precision than float32\n",
    "        # bfloat16: 16 bits, 1 sign bit, 8 exponent bits, 7 mantissa bits\n",
    "        # float16: 16 bits, 1 sign bit, 5 exponent bits, 10 mantissa bits\n",
    "        # float32: 32 bits, 1 sign bit, 8 exponent bits, 23 mantissa bits\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=deviceType, dtype=torch.bfloat16):\n",
    "            embeddings = ctx_encoder_gpu(**inputs).pooler_output\n",
    "            embeddings_cpu = embeddings.detach().cpu().to(torch.float32).tolist()\n",
    "            # print(f\"Process {process_id}: GPU memory usage: {torch.cuda.memory_allocated(device) / (1024 ** 3):.2f} GB\")\n",
    "            # print(f\"Process {process_id}: GPU allocated memory: {torch.cuda.memory_reserved(device) / (1024 ** 3):.2f} GB\")\n",
    "            return {\"embedding\": embeddings_cpu}\n",
    "    \n",
    "    # Process the shard\n",
    "    try:\n",
    "        shard_directory = CORPUS_CACHE_DIR / f\"shard_{process_id}\"\n",
    "        shard_directory.mkdir(parents=True, exist_ok=True)\n",
    "        shard.map(\n",
    "            embed_codes_gpu,\n",
    "            batched=True,\n",
    "            batch_size=48,\n",
    "            desc=f\"Process {process_id}\",\n",
    "            cache_file_name=str(shard_directory / f\"shard_{process_id}\"),\n",
    "        )\n",
    "\n",
    "        print(f\"Process {process_id}: Successfully processed {len(shard)} documents\")\n",
    "        \n",
    "        # Save the processed dataset to a specific location for later loading\n",
    "        # processed_shard.save_to_disk(str(shard_cache_dir))\n",
    "        \n",
    "        # Ensure shard directory has content\n",
    "        if not any(shard_directory.iterdir()):\n",
    "            raise FileNotFoundError(f\"Processed shard file not found: {shard_directory}\")\n",
    "        \n",
    "        return str(shard_directory)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Process {process_id}: Error during processing: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93421bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_processpool(corpus: Dataset):\n",
    "    \"\"\"\n",
    "    Process the dataset using a process pool for true parallel execution.\n",
    "    Each process gets its own CUDA context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    num_processes = num_gpus * 2  # Use more GPUs per process for better load balancing\n",
    "    print(f\"Found {num_gpus} GPUs available\")\n",
    "    \n",
    "    if num_gpus < 1:\n",
    "        raise RuntimeError(\"At least one GPU is required for this operation.\")\n",
    "\n",
    "    # Calculate shard sizes\n",
    "    total_docs = len(corpus)\n",
    "    docs_per_process = total_docs // num_processes\n",
    "    remainder = total_docs % num_processes\n",
    "\n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    print(f\"Documents per process: {docs_per_process}\")\n",
    "    print(f\"Remainder documents: {remainder}\")\n",
    "    \n",
    "    # Create shards and distribute workload across Processes\n",
    "    shards = []\n",
    "    start_idx = 0\n",
    "    for process_id in range(num_processes):\n",
    "        # Give remainder documents to first few processes\n",
    "        shard_size = docs_per_process + (1 if process_id < remainder else 0)\n",
    "        end_idx = start_idx + shard_size\n",
    "        \n",
    "        shard = corpus.select(range(start_idx, end_idx))\n",
    "        shards.append((process_id, shard))\n",
    "\n",
    "        print(f\"Process {process_id}: Processing documents {start_idx} to {end_idx-1} ({shard_size} docs)\")\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Process shards in parallel using processes\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        futures = []\n",
    "        for process_id, shard in shards:\n",
    "            future = executor.submit(process_shard_on_gpu, process_id, shard)\n",
    "            futures.append((process_id, future))\n",
    "\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Starting parallel processing on {num_processes} Processes...\")        \n",
    "        wait(futures)  # Wait for all futures to complete\n",
    "    \n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] All Processes completed processing!\")\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Please wait for the main process to combine results...\")    \n",
    "        shard_results: list[Dataset] = []\n",
    "        for process_id, future in futures:\n",
    "            shard_dataset_path: str = future.result()\n",
    "            shard_results.append(Dataset.load_from_disk(shard_dataset_path))\n",
    "    \n",
    "    # Combine all shard results\n",
    "    print(\"Combining results from all Processes...\")\n",
    "\n",
    "    # Create final dataset\n",
    "    corpus_with_embeddings = concatenate_datasets(shard_results)\n",
    "    \n",
    "    print(f\"Combined dataset created with {len(corpus_with_embeddings)} documents\")\n",
    "    return corpus_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cache exists and load, otherwise process corpus\n",
    "if CORPUS_CACHE_DIR.exists():\n",
    "    try:\n",
    "        print(f\"Loading corpus cache from {CORPUS_CACHE_DIR}\")\n",
    "        corpus_with_embeddings = Dataset.load_from_disk(str(CORPUS_CACHE_DIR))\n",
    "        print(f\"Cache loaded successfully. Documents: {len(corpus_with_embeddings)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load cache: {e}\")\n",
    "        print(\"Cache directory exists but contains invalid data. Recreating cache...\")\n",
    "        corpus_with_embeddings = None\n",
    "else:\n",
    "    corpus_with_embeddings = None\n",
    "\n",
    "if corpus_with_embeddings is None:\n",
    "    print(\"No cache found. Processing corpus...\")\n",
    "    \n",
    "    # Load corpus dataset\n",
    "    corpus = load_dataset(\n",
    "        \"NTU-NLP-sg/xCodeEval\",\n",
    "        \"retrieval_corpus\",\n",
    "        trust_remote_code=True,\n",
    "        split=\"test\",\n",
    "        revision=\"467d25a839086383794b58055981221b82c0d107\",\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    \n",
    "    # Generate embeddings\n",
    "    corpus_with_embeddings = process_with_processpool(corpus)\n",
    "    \n",
    "    print(\"Embeddings generated successfully!\")\n",
    "    print(f\"Saving corpus cache to {CORPUS_CACHE_DIR}\")\n",
    "    corpus_with_embeddings.save_to_disk(str(CORPUS_CACHE_DIR))\n",
    "    print(\"Cache saved successfully!\")\n",
    "\n",
    "# Display information about the processed corpus\n",
    "print(f\"\\nCorpus information:\")\n",
    "print(f\"Number of documents: {len(corpus_with_embeddings)}\")\n",
    "if len(corpus_with_embeddings) > 0:\n",
    "    print(f\"Embedding dimension: {len(corpus_with_embeddings[0]['embedding'])}\")\n",
    "    print(f\"Sample document keys: {list(corpus_with_embeddings[0].keys())}\")\n",
    "    print(f\"Sample source code (first 200 chars): {corpus_with_embeddings[0]['source_code'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
