{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d6b9aa",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91e8958",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (4.53.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (0.33.2)\n",
      "Requirement already satisfied: omegaconf in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: datasets==2.16.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: tqdm in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.12.13)\n",
      "Requirement already satisfied: packaging in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.16.1) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: nbconvert in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (7.16.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (5.8.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (25.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbconvert) (2.19.2)\n",
      "Requirement already satisfied: webencodings in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
      "Requirement already satisfied: decorator in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: stack_data in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert) (4.3.8)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (4.24.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.26.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat>=5.7->nbconvert) (4.14.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from beautifulsoup4->nbconvert) (2.7)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/developer/nlp-tf/.conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - conda-forge\n",
      " - pytorch\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install python dependencies\n",
    "%pip install torch transformers huggingface_hub omegaconf datasets==2.16.1 tqdm \n",
    "# Optinal python packages for better user experience\n",
    "%pip install ipywidgets nbconvert\n",
    "# Install conda dependencies\n",
    "%conda install -c conda-forge -c pytorch -c nvidia faiss-gpu=1.11.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ceb26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import omegaconf\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, AutoTokenizer, DPRConfig, GPT2TokenizerFast\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Setup external services authentication\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372f5df",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys_substring(ordered_dict: OrderedDict[str, Any], find_pattern, replace_pattern):\n",
    "    \"\"\"\n",
    "    Rename keys in an OrderedDict by replacing substring occurrences using regular expressions.\n",
    "    \n",
    "    Args:\n",
    "        ordered_dict: The OrderedDict to modify\n",
    "        find_pattern: The regex pattern to find in keys\n",
    "        replace_pattern: The replacement pattern (can include backreferences like \\\\1, \\\\2)\n",
    "    \n",
    "    Returns:\n",
    "        New Mapping with renamed keys\n",
    "    \"\"\"\n",
    "    new_dict = OrderedDict[str, Any]()\n",
    "    compiled_pattern = re.compile(find_pattern)\n",
    "    \n",
    "    for key, value in ordered_dict.items():\n",
    "        if not compiled_pattern.search(key):\n",
    "            continue\n",
    "            \n",
    "        new_key = compiled_pattern.sub(replace_pattern, key)\n",
    "        new_dict[new_key] = value\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc881537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the necessary types are registered for safe deserialization\n",
    "torch.serialization.add_safe_globals(\n",
    "    [\n",
    "        omegaconf.dictconfig.ContainerMetadata,\n",
    "        omegaconf.dictconfig.DictConfig,\n",
    "        omegaconf.base.Metadata,\n",
    "        omegaconf.nodes.AnyNode,\n",
    "        omegaconf.listconfig.ListConfig,\n",
    "        collections.defaultdict,\n",
    "        Any,\n",
    "        dict,\n",
    "        list,\n",
    "        int,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model from checkpoint\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"NTU-NLP-sg/xCodeEval-nl-code-starencoder-ckpt-37\",\n",
    "    filename=\"dpr_biencoder.37.pt\",\n",
    "    repo_type=\"model\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Retrieve fine-tuned weights\n",
    "# Pattern matches: question_model.embeddings/encoder.* -> question_encoder.bert_model.*\n",
    "question_state_dict = rename_keys_substring(\n",
    "    state_dict[\"model_dict\"],\n",
    "    r\"question_model\\.(embeddings|encoder)\\.([Ll]ayer|token|word|position_embeddings)\",\n",
    "    r\"question_encoder.bert_model.\\1.\\2\",\n",
    ")\n",
    "ctx_state_dict = rename_keys_substring(\n",
    "    state_dict[\"model_dict\"],\n",
    "    r\"ctx_model\\.(embeddings|encoder)\\.([Ll]ayer|token|word|position_embeddings)\",\n",
    "    r\"ctx_encoder.bert_model.\\1.\\2\",\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize encoders\n",
    "pretrained_model_name = state_dict[\"encoder_params\"][\"encoder\"][\"pretrained_model_cfg\"]\n",
    "encoder_config = DPRConfig.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    None, state_dict=question_state_dict, config=encoder_config, token=HF_TOKEN\n",
    ")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\n",
    "    None, state_dict=ctx_state_dict, config=encoder_config, token=HF_TOKEN\n",
    ")\n",
    "\n",
    "question_encoder = question_encoder.to(device).eval()\n",
    "ctx_encoder = ctx_encoder.to(device).eval()\n",
    "\n",
    "# kwargs (additional keyword arguments, optional):\n",
    "\n",
    "# Will be passed to the Tokenizer __init__() method. Can be used to set special tokens like\n",
    "# bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token,\n",
    "# additional_special_tokens. See parameters in the __init__() for more details.\n",
    "\n",
    "\n",
    "tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name, config=encoder_config\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c78fbc",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e230e79ea64c2ca5da4085be1a1ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25043700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'numpy.ndarray'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'torch.Tensor'>)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m: ctx_encoder(**inputs).pooler_output.cpu().numpy()}\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# corpus.add_faiss_index\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# This automatically stores embeddings in the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m corpus = \u001b[43mcorpus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m48\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m i = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:592\u001b[39m, in \u001b[36mtransmit_tasks.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m    595\u001b[39m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3093\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3088\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3089\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3090\u001b[39m         total=pbar_total,\n\u001b[32m   3091\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3092\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset._map_single(**dataset_kwargs):\n\u001b[32m   3094\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m   3095\u001b[39m                 shards_done += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3470\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3466\u001b[39m indices = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   3467\u001b[39m     \u001b[38;5;28mrange\u001b[39m(*(\u001b[38;5;28mslice\u001b[39m(i, i + batch_size).indices(shard.num_rows)))\n\u001b[32m   3468\u001b[39m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[32m   3469\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3470\u001b[39m     batch = \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3474\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3476\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[32m   3477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[32m   3478\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3479\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3360\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[39m\u001b[34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[39m\n\u001b[32m   3357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3358\u001b[39m     \u001b[38;5;66;03m# Check if the function returns updated examples\u001b[39;00m\n\u001b[32m   3359\u001b[39m     update_data = \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa.Table, pd.DataFrame))\n\u001b[32m-> \u001b[39m\u001b[32m3360\u001b[39m     \u001b[43mvalidate_function_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update_data:\n\u001b[32m   3362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Nothing to update, let's move on\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nlp-tf/.conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3326\u001b[39m, in \u001b[36mDataset._map_single.<locals>.validate_function_output\u001b[39m\u001b[34m(processed_inputs, indices)\u001b[39m\n\u001b[32m   3322\u001b[39m all_dict_values_are_lists = \u001b[38;5;28mall\u001b[39m(\n\u001b[32m   3323\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(value, allowed_batch_return_types) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m processed_inputs.values()\n\u001b[32m   3324\u001b[39m )\n\u001b[32m   3325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_dict_values_are_lists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3327\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProvided `function` which is applied to all elements of table returns a `dict` of types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mprocessed_inputs.values()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. When using `batched=True`, make sure provided `function` returns a `dict` of types like `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_batch_return_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3328\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'numpy.ndarray'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'torch.Tensor'>)`."
     ]
    }
   ],
   "source": [
    "# Load NL-Code retrieval data\n",
    "nl_code_test = load_dataset(\n",
    "    \"NTU-NLP-sg/xCodeEval\",\n",
    "    \"retrieval_nl_code\",\n",
    "    trust_remote_code=True,\n",
    "    split=\"test\",\n",
    "    revision=\"467d25a839086383794b58055981221b82c0d107\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "corpus = load_dataset(\n",
    "    \"NTU-NLP-sg/xCodeEval\",\n",
    "    \"retrieval_corpus\",\n",
    "    trust_remote_code=True,\n",
    "    split=\"test\",\n",
    "    revision=\"467d25a839086383794b58055981221b82c0d107\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "def embed_codes(batch):\n",
    "    inputs = tokenizer(\n",
    "        batch[\"source_code\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"np\"  # Directly get NumPy arrays\n",
    "    )\n",
    "    inputs = {k: torch.tensor(v).to(device) for k,v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        return {\"embedding\": ctx_encoder(**inputs).pooler_output.cpu().numpy().tolist()}\n",
    "    \n",
    "    \n",
    "corpus = corpus.map(embed_codes, batched=True, batch_size=48)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GPU-accelerated FAISS index\n",
    "embeddings = corpus[\"embedding\"]\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, index)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk(k_vals=[1, 5, 10, 100]):\n",
    "    results = {k: 0 for k in k_vals}\n",
    "    \n",
    "    for example in tqdm(nl_code_test):\n",
    "        # Encode NL query\n",
    "        inputs = tokenizer(\n",
    "            example[\"nl\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            query_embed = question_encoder(**inputs).pooler_output.cpu().numpy()\n",
    "        \n",
    "        # Retrieve top-k results\n",
    "        distances, indices = index.search(query_embed, max(k_vals))\n",
    "        \n",
    "        # Check for relevant matches\n",
    "        retrieved_uids = [src_uids[i] for i in indices[0]]\n",
    "        for k in k_vals:\n",
    "            if example[\"src_uid\"] in retrieved_uids[:k]:\n",
    "                results[k] += 1\n",
    "                \n",
    "    # Calculate accuracy\n",
    "    total = len(nl_code_test)\n",
    "    return {k: v / total for k, v in results.items()}\n",
    "\n",
    "# Run evaluation\n",
    "topk_acc = evaluate_topk()\n",
    "print(\"Top-K Accuracy:\", topk_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
